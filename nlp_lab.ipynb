{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+7MH0tchIwxl73n96qZGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manu-eldho/344/blob/main/nlp_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concordance"
      ],
      "metadata": {
        "id": "IOtrIiW3Wgd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5AJsfCwW6ZP",
        "outputId": "422fab4e-3046-444b-dd00-cbf5a579f8ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiR99o8QWbx1",
        "outputId": "4626a2a1-842b-4ace-b827-857aabb51be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the word to find concordance for: life\n",
            "Enter the window size (e.g., 40): 10\n",
            "\n",
            "Concordance for 'life' with window size 10:\n",
            "\n",
            "Displaying 5 of 5 matches:\n",
            "   Life is\n",
            " . Life ca\n",
            "'s life ha\n",
            "he life of\n",
            "er life to\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.text import Text\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "custom_text = \"\"\"\n",
        "Life is what happens when you're busy making other plans.\n",
        "Life can be beautiful if you know how to live it.\n",
        "Everyone's life has a purpose and a meaning.\n",
        "The life of a sailor is full of adventure.\n",
        "She dedicated her life to science and discovery.\n",
        "\"\"\"\n",
        "\n",
        "tokens = word_tokenize(custom_text)\n",
        "text_obj = Text(tokens)\n",
        "word = input(\"Enter the word to find concordance for: \")\n",
        "width = int(input(\"Enter the window size (e.g., 40): \"))\n",
        "print(f\"\\nConcordance for '{word}' with window size {width}:\\n\")\n",
        "text_obj.concordance(word, width=width)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "custom_text = \"\"\"\n",
        "Life is what happens when you're busy making other plans.\n",
        "Life can be beautiful if you know how to live it.\n",
        "Everyone's life has a purpose and a meaning.\n",
        "The life of a sailor is full of adventure.\n",
        "She dedicated her life to science and discovery.\n",
        "\"\"\"\n",
        "tokens = word_tokenize(custom_text)\n",
        "target_word = input(\"Enter the word to search: \").lower()\n",
        "window_size = int(input(\"Enter the window size: \"))\n",
        "tokens_lower = [word.lower() for word in tokens]\n",
        "print(f\"\\nConcordance for '{target_word}' with window size {window_size}:\\n\")\n",
        "\n",
        "for i, word in enumerate(tokens_lower):\n",
        "    if word == target_word:\n",
        "        left = tokens[max(0, i - window_size): i]\n",
        "        right = tokens[i + 1: i + 1 + window_size]\n",
        "        center = tokens[i]\n",
        "        print(\"... \" + ' '.join(left) + \" [\" + center + \"] \" + ' '.join(right) + \" ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa5L2Qu2XirH",
        "outputId": "383e883e-a6a1-4dd0-9f3b-48d4567017f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the word to search: life\n",
            "Enter the window size: 2\n",
            "\n",
            "Concordance for 'life' with window size 2:\n",
            "\n",
            "...  [Life] is what ...\n",
            "... plans . [Life] can be ...\n",
            "... Everyone 's [life] has a ...\n",
            "... . The [life] of a ...\n",
            "... dedicated her [life] to science ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counting Vocabulary"
      ],
      "metadata": {
        "id": "MjyjJt5NYU4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "custom_text = \"\"\"\n",
        "Life is what happens when you're busy making other plans.\n",
        "Life can be beautiful if you know how to live it.\n",
        "Everyone's life has a purpose and a meaning.\n",
        "The life of a sailor is full of adventure.\n",
        "She dedicated her life to science and discovery.\n",
        "\"\"\"\n",
        "\n",
        "tokens = word_tokenize(custom_text.lower())  # Normalize to lowercase\n",
        "total_tokens = len(tokens)\n",
        "unique_tokens = len(set(tokens))\n",
        "\n",
        "# ----------- Frequency Analysis ------------\n",
        "freq_dist = Counter(tokens)\n",
        "the_count = freq_dist[\"the\"]\n",
        "the_percentage = (the_count / total_tokens) * 100\n",
        "ttr = unique_tokens / total_tokens\n",
        "\n",
        "# ----------- Display Results ------------\n",
        "print(f\"1. Total number of words (tokens): {total_tokens}\")\n",
        "print(f\"2. Number of different words (types): {unique_tokens}\")\n",
        "print(f\"3. Occurrences of the word 'the': {the_count}\")\n",
        "print(f\"4. Percentage of 'the' in the text: {the_percentage:.2f}%\")\n",
        "print(f\"5. Type-Token Ratio (TTR): {ttr:.3f}\")\n",
        "print(\"\\n6. Occurrence of each word in the text:\\n\")\n",
        "\n",
        "# ----------- Print Word Frequencies ------------\n",
        "for word, count in freq_dist.items():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68607FQEYd6R",
        "outputId": "00c40341-f98c-44b6-ff93-a05b8a4a0b69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'life': 5, '.': 5, 'a': 3, 'is': 2, 'you': 2, 'to': 2, 'and': 2, 'of': 2, 'what': 1, 'happens': 1, 'when': 1, \"'re\": 1, 'busy': 1, 'making': 1, 'other': 1, 'plans': 1, 'can': 1, 'be': 1, 'beautiful': 1, 'if': 1, 'know': 1, 'how': 1, 'live': 1, 'it': 1, 'everyone': 1, \"'s\": 1, 'has': 1, 'purpose': 1, 'meaning': 1, 'the': 1, 'sailor': 1, 'full': 1, 'adventure': 1, 'she': 1, 'dedicated': 1, 'her': 1, 'science': 1, 'discovery': 1})\n",
            "1. Total number of words (tokens): 53\n",
            "2. Number of different words (types): 38\n",
            "3. Occurrences of the word 'the': 1\n",
            "4. Percentage of 'the' in the text: 1.89%\n",
            "5. Type-Token Ratio (TTR): 0.717\n",
            "\n",
            "6. Occurrence of each word in the text:\n",
            "\n",
            "life: 5\n",
            "is: 2\n",
            "what: 1\n",
            "happens: 1\n",
            "when: 1\n",
            "you: 2\n",
            "'re: 1\n",
            "busy: 1\n",
            "making: 1\n",
            "other: 1\n",
            "plans: 1\n",
            ".: 5\n",
            "can: 1\n",
            "be: 1\n",
            "beautiful: 1\n",
            "if: 1\n",
            "know: 1\n",
            "how: 1\n",
            "to: 2\n",
            "live: 1\n",
            "it: 1\n",
            "everyone: 1\n",
            "'s: 1\n",
            "has: 1\n",
            "a: 3\n",
            "purpose: 1\n",
            "and: 2\n",
            "meaning: 1\n",
            "the: 1\n",
            "of: 2\n",
            "sailor: 1\n",
            "full: 1\n",
            "adventure: 1\n",
            "she: 1\n",
            "dedicated: 1\n",
            "her: 1\n",
            "science: 1\n",
            "discovery: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "cv9b8K1gZgPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ----------- Custom Text ------------\n",
        "text = \"\"\"\n",
        "Life is what happens when you're busy making other plans.\n",
        "Life can be beautiful if you know how to live it.\n",
        "Everyone's life has a purpose and a meaning.\n",
        "The life of a sailor is full of adventure.\n",
        "She dedicated her life to science and discovery.\n",
        "\"\"\"\n",
        "\n",
        "# ----------- Step 1: Tokenization ------------\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# ----------- Step 2: Filtration (remove punctuation, numbers) ------------\n",
        "filtered_tokens = [word for word in tokens if word.isalpha()]\n",
        "print(\"\\nAfter Filtration (alphabetic only):\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "# ----------- Step 3: Stop Word Removal ------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [word for word in filtered_tokens if word.lower() not in stop_words]\n",
        "print(\"\\nAfter Stop Word Removal:\")\n",
        "print(tokens_no_stopwords)\n",
        "\n",
        "# ----------- Step 4: Stemming ------------\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(stemmed)\n",
        "\n",
        "# ----------- Step 5: Lemmatization ------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokens_no_stopwords]\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa2BFF7taAkm",
        "outputId": "20135ce0-603b-4e07-9c6c-0dd644ddc774"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Life', 'is', 'what', 'happens', 'when', 'you', \"'re\", 'busy', 'making', 'other', 'plans', '.', 'Life', 'can', 'be', 'beautiful', 'if', 'you', 'know', 'how', 'to', 'live', 'it', '.', 'Everyone', \"'s\", 'life', 'has', 'a', 'purpose', 'and', 'a', 'meaning', '.', 'The', 'life', 'of', 'a', 'sailor', 'is', 'full', 'of', 'adventure', '.', 'She', 'dedicated', 'her', 'life', 'to', 'science', 'and', 'discovery', '.']\n",
            "\n",
            "After Filtration (alphabetic only):\n",
            "['Life', 'is', 'what', 'happens', 'when', 'you', 'busy', 'making', 'other', 'plans', 'Life', 'can', 'be', 'beautiful', 'if', 'you', 'know', 'how', 'to', 'live', 'it', 'Everyone', 'life', 'has', 'a', 'purpose', 'and', 'a', 'meaning', 'The', 'life', 'of', 'a', 'sailor', 'is', 'full', 'of', 'adventure', 'She', 'dedicated', 'her', 'life', 'to', 'science', 'and', 'discovery']\n",
            "\n",
            "After Stop Word Removal:\n",
            "['Life', 'happens', 'busy', 'making', 'plans', 'Life', 'beautiful', 'know', 'live', 'Everyone', 'life', 'purpose', 'meaning', 'life', 'sailor', 'full', 'adventure', 'dedicated', 'life', 'science', 'discovery']\n",
            "\n",
            "After Stemming:\n",
            "['life', 'happen', 'busi', 'make', 'plan', 'life', 'beauti', 'know', 'live', 'everyon', 'life', 'purpos', 'mean', 'life', 'sailor', 'full', 'adventur', 'dedic', 'life', 'scienc', 'discoveri']\n",
            "\n",
            "After Lemmatization:\n",
            "['life', 'happens', 'busy', 'making', 'plan', 'life', 'beautiful', 'know', 'live', 'everyone', 'life', 'purpose', 'meaning', 'life', 'sailor', 'full', 'adventure', 'dedicated', 'life', 'science', 'discovery']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import unicodedata\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ----------- Custom Text ------------\n",
        "text = \"\"\"\n",
        "Life is what happens when you're busy making other plans.\n",
        "äººç”Ÿã¯ã€ä»–ã®è¨ˆç”»ã‚’ç«‹ã¦ã¦ã„ã‚‹ã¨ãã«èµ·ã“ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
        "Life can be beautiful if you know how to live it.\n",
        "Everyone's life has a purpose and a meaning.\n",
        "The life of a sailor is full of adventure.\n",
        "She dedicated her life to science and discovery.\n",
        "\"\"\"\n",
        "\n",
        "# ----------- Step 1: Tokenization ------------\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# ----------- Step 2: Filtration (keep only alphabetic words) ------------\n",
        "alphabetic_tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# ----------- Step 3: Script Validation (only Latin/English script) ------------\n",
        "def is_latin(word):\n",
        "    for ch in word:\n",
        "        if 'LATIN' not in unicodedata.name(ch, ''):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "validated_tokens = [word for word in alphabetic_tokens if is_latin(word)]\n",
        "print(\"\\nAfter Script Validation:\")\n",
        "print(validated_tokens)\n",
        "\n",
        "# ----------- Step 4: Stop Word Removal ------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [word for word in validated_tokens if word.lower() not in stop_words]\n",
        "print(\"\\nAfter Stop Word Removal:\")\n",
        "print(tokens_no_stopwords)\n",
        "\n",
        "# ----------- Step 5: Stemming ------------\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
        "print(\"\\nAfter Stemming:\")\n",
        "print(stemmed)\n",
        "\n",
        "# ----------- Step 6: Lemmatization ------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokens_no_stopwords]\n",
        "print(\"\\nAfter Lemmatization:\")\n",
        "print(lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckvDBo6zjylR",
        "outputId": "213cde3b-d2dc-4e42-dfde-b839d0597c37"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Life', 'is', 'what', 'happens', 'when', 'you', \"'re\", 'busy', 'making', 'other', 'plans', '.', 'äººç”Ÿã¯ã€ä»–ã®è¨ˆç”»ã‚’ç«‹ã¦ã¦ã„ã‚‹ã¨ãã«èµ·ã“ã‚‹ã‚‚ã®ã§ã™ã€‚', 'Life', 'can', 'be', 'beautiful', 'if', 'you', 'know', 'how', 'to', 'live', 'it', '.', 'Everyone', \"'s\", 'life', 'has', 'a', 'purpose', 'and', 'a', 'meaning', '.', 'The', 'life', 'of', 'a', 'sailor', 'is', 'full', 'of', 'adventure', '.', 'She', 'dedicated', 'her', 'life', 'to', 'science', 'and', 'discovery', '.']\n",
            "\n",
            "After Script Validation:\n",
            "['Life', 'is', 'what', 'happens', 'when', 'you', 'busy', 'making', 'other', 'plans', 'Life', 'can', 'be', 'beautiful', 'if', 'you', 'know', 'how', 'to', 'live', 'it', 'Everyone', 'life', 'has', 'a', 'purpose', 'and', 'a', 'meaning', 'The', 'life', 'of', 'a', 'sailor', 'is', 'full', 'of', 'adventure', 'She', 'dedicated', 'her', 'life', 'to', 'science', 'and', 'discovery']\n",
            "\n",
            "After Stop Word Removal:\n",
            "['Life', 'happens', 'busy', 'making', 'plans', 'Life', 'beautiful', 'know', 'live', 'Everyone', 'life', 'purpose', 'meaning', 'life', 'sailor', 'full', 'adventure', 'dedicated', 'life', 'science', 'discovery']\n",
            "\n",
            "After Stemming:\n",
            "['life', 'happen', 'busi', 'make', 'plan', 'life', 'beauti', 'know', 'live', 'everyon', 'life', 'purpos', 'mean', 'life', 'sailor', 'full', 'adventur', 'dedic', 'life', 'scienc', 'discoveri']\n",
            "\n",
            "After Lemmatization:\n",
            "['life', 'happens', 'busy', 'making', 'plan', 'life', 'beautiful', 'know', 'live', 'everyone', 'life', 'purpose', 'meaning', 'life', 'sailor', 'full', 'adventure', 'dedicated', 'life', 'science', 'discovery']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing"
      ],
      "metadata": {
        "id": "0NlcdlcljbDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import CFG, PCFG\n",
        "from nltk.parse import ChartParser\n",
        "from nltk.parse import pchart\n",
        "\n",
        "sentence = \"the cat chased the mouse\".split()\n",
        "\n",
        "cfg_grammar = CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det N\n",
        "VP -> V NP\n",
        "Det -> 'the'\n",
        "N -> 'cat' | 'mouse'\n",
        "V -> 'chased'\n",
        "\"\"\")\n",
        "\n",
        "print(\"=== Constituency Parsing (CFG) ===\")\n",
        "cfg_parser = ChartParser(cfg_grammar)\n",
        "for tree in cfg_parser.parse(sentence):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n",
        "\n",
        "pcfg_grammar = PCFG.fromstring(\"\"\"\n",
        "S -> NP VP [1.0]\n",
        "NP -> Det N [1.0]\n",
        "VP -> V NP [1.0]\n",
        "Det -> 'the' [1.0]\n",
        "N -> 'cat' [0.5] | 'mouse' [0.5]\n",
        "V -> 'chased' [1.0]\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n=== Probabilistic Parsing (PCFG) ===\")\n",
        "viterbi_parser = pchart.InsideChartParser(pcfg_grammar)\n",
        "for tree in viterbi_parser.parse(sentence):\n",
        "    print(tree)\n",
        "    tree.pretty_print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5DAakQMjfn5",
        "outputId": "e43e5bc6-4904-4351-e788-2a4674d0f673"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Constituency Parsing (CFG) ===\n",
            "(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det the) (N mouse))))\n",
            "              S                 \n",
            "      ________|_____             \n",
            "     |              VP          \n",
            "     |         _____|___         \n",
            "     NP       |         NP      \n",
            "  ___|___     |      ___|____    \n",
            "Det      N    V    Det       N  \n",
            " |       |    |     |        |   \n",
            "the     cat chased the     mouse\n",
            "\n",
            "\n",
            "=== Probabilistic Parsing (PCFG) ===\n",
            "(S\n",
            "  (NP (Det the) (N cat))\n",
            "  (VP (V chased) (NP (Det the) (N mouse)))) (p=0.25)\n",
            "              S                 \n",
            "      ________|_____             \n",
            "     |              VP          \n",
            "     |         _____|___         \n",
            "     NP       |         NP      \n",
            "  ___|___     |      ___|____    \n",
            "Det      N    V    Det       N  \n",
            " |       |    |     |        |   \n",
            "the     cat chased the     mouse\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words"
      ],
      "metadata": {
        "id": "GKtVpiNdmSBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"I love Natural Language Processing!\",\n",
        "    \"Language processing with Python is fun.\",\n",
        "    \"Natural language processing includes text mining and NLP.\"\n",
        "]\n",
        "\n",
        "# Step 1: Vectorization using Bag of Words\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english', token_pattern=r'\\b\\w+\\b')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Convert to DataFrame (for viewing)\n",
        "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Step 3: Compute Cosine Similarity\n",
        "cosine_sim = cosine_similarity(X)\n",
        "\n",
        "# Step 4: Display\n",
        "print(\"Vocabulary:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\n Bag of Words (Document-Term Matrix):\")\n",
        "print(bow_df)\n",
        "\n",
        "print(\"\\n Cosine Similarity Between Sentences:\")\n",
        "cosine_df = pd.DataFrame(cosine_sim, index=[f\"Doc{i+1}\" for i in range(len(documents))],\n",
        "                                      columns=[f\"Doc{i+1}\" for i in range(len(documents))])\n",
        "print(cosine_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYABAJuQmVvJ",
        "outputId": "c01ea83a-97a8-47ed-f5c2-a2c91a495843"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['fun' 'includes' 'language' 'love' 'mining' 'natural' 'nlp' 'processing'\n",
            " 'python' 'text']\n",
            "\n",
            "ðŸ§¾ Bag of Words (Document-Term Matrix):\n",
            "   fun  includes  language  love  mining  natural  nlp  processing  python  \\\n",
            "0    0         0         1     1       0        1    0           1       0   \n",
            "1    1         0         1     0       0        0    0           1       1   \n",
            "2    0         1         1     0       1        1    1           1       0   \n",
            "\n",
            "   text  \n",
            "0     0  \n",
            "1     0  \n",
            "2     1  \n",
            "\n",
            "ðŸ“ Cosine Similarity Between Sentences:\n",
            "          Doc1      Doc2      Doc3\n",
            "Doc1  1.000000  0.500000  0.566947\n",
            "Doc2  0.500000  1.000000  0.377964\n",
            "Doc3  0.566947  0.377964  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar Sentence"
      ],
      "metadata": {
        "id": "wqMs7VZ3qPcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 1: Read the file\n",
        "filename = \"/content/similar.txt\"  # Ensure this file exists\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    sentences = [line.strip() for line in file if line.strip()]\n",
        "\n",
        "# Step 2: Get user input\n",
        "input_sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "# Step 3: Add the input sentence to the list\n",
        "all_sentences = sentences + [input_sentence]\n",
        "\n",
        "# Step 4: Vectorize using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
        "\n",
        "# Step 5: Compute cosine similarity between input sentence and all others\n",
        "similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
        "\n",
        "# Step 6: Find the index of the most similar sentence\n",
        "most_similar_index = similarities.argmax()\n",
        "most_similar_score = similarities[0][most_similar_index]\n",
        "most_similar_sentence = sentences[most_similar_index]\n",
        "\n",
        "# Step 7: Print result\n",
        "print(\"\\nMost similar sentence from the file:\")\n",
        "print(f\"> {most_similar_sentence}\")\n",
        "print(f\"\\nCosine Similarity Score: {most_similar_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgCEclTFpRxD",
        "outputId": "ef117445-d509-4689-926b-03e8c08263d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: natural language\n",
            "\n",
            "Most similar sentence from the file:\n",
            "> Natural language processing is a field of artificial intelligence.\n",
            "\n",
            "Cosine Similarity Score: 0.4168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NER"
      ],
      "metadata": {
        "id": "9wQe7kClq0-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhRI5RH_rpFR",
        "outputId": "05a9d1ca-939c-4009-ad53-c6a3821a18ac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaDH3II_r1bO",
        "outputId": "e5087081-42c5-4802-d4ab-3ef31c3653bb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "text = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities in the text:\\n\")\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vsY6JxOrcp8",
        "outputId": "4409c7e6-b298-4158-8b81-139b2c5e5d8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hawaii', 'NNP'), ('and', 'CC'), ('served', 'VBD'), ('as', 'IN'), ('the', 'DT'), ('44th', 'CD'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
            "Named Entities in the text:\n",
            "\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  and/CC\n",
            "  served/VBD\n",
            "  as/IN\n",
            "  the/DT\n",
            "  44th/CD\n",
            "  President/NNP\n",
            "  of/IN\n",
            "  the/DT\n",
            "  (GPE United/NNP States/NNPS)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Download only what's necessary\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "# Input text\n",
        "text = \"Elon Musk founded SpaceX and Tesla in the United States.\"\n",
        "\n",
        "# Tokenize and tag\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "# Named Entity Recognition\n",
        "tree = ne_chunk(tags)\n",
        "\n",
        "# Display named entities\n",
        "for subtree in tree:\n",
        "    if isinstance(subtree, nltk.Tree):\n",
        "        entity = \" \".join(word for word, tag in subtree.leaves())\n",
        "        entity_type = subtree.label()\n",
        "        print(f\"{entity} => {entity_type}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5k16JcEt10e",
        "outputId": "887e25b0-4e2e-47af-bc4d-0a5e503e66cb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon => PERSON\n",
            "Musk => PERSON\n",
            "SpaceX => ORGANIZATION\n",
            "Tesla => GPE\n",
            "United States => GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "OtOKBjaLwPiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The sky is blue.\",\n",
        "    \"The sun is bright.\",\n",
        "    \"The sun in the sky is bright.\",\n",
        "    \"We can see the shining sun, the bright sun.\"\n",
        "]\n",
        "\n",
        "# Create the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the TF-IDF scores as a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the TF-IDF values\n",
        "print(\"TF-IDF values for each word in each document:\\n\")\n",
        "print(tfidf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYZ6fpQPwS5O",
        "outputId": "d243b96a-8232-4186-c586-90eb2d23d930"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF values for each word in each document:\n",
            "\n",
            "       blue    bright       can        in        is       see   shining  \\\n",
            "0  0.659191  0.000000  0.000000  0.000000  0.420753  0.000000  0.000000   \n",
            "1  0.000000  0.522109  0.000000  0.000000  0.522109  0.000000  0.000000   \n",
            "2  0.000000  0.321846  0.000000  0.504235  0.321846  0.000000  0.000000   \n",
            "3  0.000000  0.239102  0.374599  0.000000  0.000000  0.374599  0.374599   \n",
            "\n",
            "        sky       sun       the        we  \n",
            "0  0.519714  0.000000  0.343993  0.000000  \n",
            "1  0.000000  0.522109  0.426858  0.000000  \n",
            "2  0.397544  0.321846  0.526261  0.000000  \n",
            "3  0.000000  0.478204  0.390963  0.374599  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tagging"
      ],
      "metadata": {
        "id": "VGntpQX2wafd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import hmm\n",
        "from nltk.probability import LaplaceProbDist\n",
        "\n",
        "nltk.download('treebank')\n",
        "\n",
        "tag_expansion = {\n",
        "    'CC': 'Coordinating Conjunction',\n",
        "    'CD': 'Cardinal Digit',\n",
        "    'DT': 'Determiner',\n",
        "    'EX': 'Existential There',\n",
        "    'FW': 'Foreign Word',\n",
        "    'IN': 'Preposition/Subordinating Conjunction',\n",
        "    'JJ': 'Adjective',\n",
        "    'JJR': 'Adjective, Comparative',\n",
        "    'JJS': 'Adjective, Superlative',\n",
        "    'LS': 'List Item Marker',\n",
        "    'MD': 'Modal',\n",
        "    'NN': 'Noun, Singular',\n",
        "    'NNS': 'Noun, Plural',\n",
        "    'NNP': 'Proper Noun, Singular',\n",
        "    'NNPS': 'Proper Noun, Plural',\n",
        "    'PDT': 'Predeterminer',\n",
        "    'POS': 'Possessive Ending',\n",
        "    'PRP': 'Personal Pronoun',\n",
        "    'PRP$': 'Possessive Pronoun',\n",
        "    'RB': 'Adverb',\n",
        "    'RBR': 'Adverb, Comparative',\n",
        "    'RBS': 'Adverb, Superlative',\n",
        "    'RP': 'Particle',\n",
        "    'SYM': 'Symbol',\n",
        "    'TO': 'To',\n",
        "    'UH': 'Interjection',\n",
        "    'VB': 'Verb, Base Form',\n",
        "    'VBD': 'Verb, Past Tense',\n",
        "    'VBG': 'Verb, Gerund or Present Participle',\n",
        "    'VBN': 'Verb, Past Participle',\n",
        "    'VBP': 'Verb, Non-3rd Person Singular Present',\n",
        "    'VBZ': 'Verb, 3rd Person Singular Present',\n",
        "    'WDT': 'Wh-determiner',\n",
        "    'WP': 'Wh-pronoun',\n",
        "    'WP$': 'Possessive Wh-pronoun',\n",
        "    'WRB': 'Wh-adverb'\n",
        "}\n",
        "\n",
        "def expand_tag(tag):\t\t# Function to expand the tag\n",
        "    return tag_expansion.get(tag, \"Unknown Tag\")\n",
        "\n",
        "text = input(\"Enter a sentence: \")\t\t# Word tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\t\t# Lemmatization\n",
        "l_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized tokens:\", l_tokens)\n",
        "\n",
        "\n",
        "# Train the HMM POS Tagger using Treebank corpus\n",
        "train_sents = treebank.tagged_sents()[:4000]  \t# First 3000 sentences for training\n",
        "test_sents = treebank.tagged_sents()[4000:]  \t# Remaining sentences for testing\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\t    # Train the HMM POS Tagger\n",
        "hmm_tagger = trainer.train(train_sents, estimator=LaplaceProbDist)\t # Use Laplace smoothing\n",
        "\n",
        "tags = hmm_tagger.tag(l_tokens)\t\t\t        # Use the trained tagger on the lemmatized tokens\n",
        "\n",
        "print(\"\\nPOS tags for the lemmatized sentence using HMM tagger:\")\n",
        "for l_token, tag in tags:\n",
        "    expanded_tag = expand_tag(tag)\n",
        "    print(f\"{l_token}: {tag} ({expanded_tag})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iR0GLYVwdDu",
        "outputId": "b70c0012-b9b2-4861-d090-6315babeadd7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: thq brown fox jumbs\n",
            "Tokens: ['thq', 'brown', 'fox', 'jumbs']\n",
            "Lemmatized tokens: ['thq', 'brown', 'fox', 'jumbs']\n",
            "\n",
            "POS tags for the lemmatized sentence using HMM tagger:\n",
            "thq: DT (Determiner)\n",
            "brown: NN (Noun, Singular)\n",
            "fox: . (Unknown Tag)\n",
            "jumbs: '' (Unknown Tag)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import hmm\n",
        "from nltk.probability import LaplaceProbDist\n",
        "\n",
        "text = input(\"Enter a sentence: \")\t\t        # Word tokenization\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\t\t        # Lemmatization\n",
        "l_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized tokens:\", l_tokens)\n",
        "\n",
        "train_sents = treebank.tagged_sents()[:4000]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train(train_sents, estimator=LaplaceProbDist)\n",
        "\n",
        "tags = hmm_tagger.tag(l_tokens)\n",
        "\n",
        "print(\"\\nPOS tags for the lemmatized sentence using HMM tagger:\")\n",
        "for l_token, tag in tags:\n",
        "    print(f\"{l_token}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501YN5O3yio4",
        "outputId": "17688a53-021b-4e5f-863c-f3c67ebccdfe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: the brown fox jumps\n",
            "Tokens: ['the', 'brown', 'fox', 'jumps']\n",
            "Lemmatized tokens: ['the', 'brown', 'fox', 'jump']\n",
            "\n",
            "POS tags for the lemmatized sentence using HMM tagger:\n",
            "the: DT\n",
            "brown: NN\n",
            "fox: .\n",
            "jump: ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Chatbot\n"
      ],
      "metadata": {
        "id": "ddpVvpdzzEEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "qa_pairs = {\n",
        "    \"hi\": [\"Hello!\", \"Hi there!\", \"Hey! How can I help you?\"],\n",
        "    \"how are you\": [\"I'm doing well, thank you!\", \"Great, and you?\"],\n",
        "    \"what is your name\": [\"I'm a simple chatbot.\", \"You can call me ChatBuddy!\"],\n",
        "    \"bye\": [\"Goodbye!\", \"See you later!\", \"Have a great day!\"],\n",
        "    \"what is python\": [\"Python is a popular programming language.\", \"A powerful, easy-to-learn programming language.\"],\n",
        "    \"who created you\": [\"I was created by a programmer using Python.\", \"That's a secret\"],\n",
        "    \"thank you\": [\"You're welcome!\", \"No problem!\", \"Glad I could help!\"]\n",
        "}\n",
        "\n",
        "def get_response(user_input):\n",
        "    user_input = user_input.lower().strip()\n",
        "    for question in qa_pairs:\n",
        "        if question in user_input:\n",
        "            return random.choice(qa_pairs[question])\n",
        "    return \"I'm not sure how to respond to that. Can you ask something else?\"\n",
        "\n",
        "print(\"ChatBot: Hello! Ask me something or type 'bye' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"bye\":\n",
        "        print(\"ChatBot:\", random.choice(qa_pairs[\"bye\"]))\n",
        "        break\n",
        "    response = get_response(user_input)\n",
        "    print(\"ChatBot:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYGjiXNGzJvs",
        "outputId": "6df10e86-e690-44bc-b00f-4ebe84fa68f5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatBot: Hello! Ask me something or type 'bye' to exit.\n",
            "You: hi\n",
            "ChatBot: Hello!\n",
            "You: what is python\n",
            "ChatBot: A powerful, easy-to-learn programming language.\n",
            "You: bye\n",
            "ChatBot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classification"
      ],
      "metadata": {
        "id": "ADDvYreF0LON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I loved the movie, it was fantastic!\",\n",
        "        \"What a terrible film. I will never watch it again.\",\n",
        "        \"Absolutely wonderful acting and storyline.\",\n",
        "        \"The plot was dull and the characters were boring.\",\n",
        "        \"An excellent movie with great performances.\",\n",
        "        \"Worst movie I've seen in years.\",\n",
        "        \"It was okay, not the best but not the worst.\",\n",
        "        \"The cinematography was beautiful, loved it.\",\n",
        "        \"The film lacked depth and emotion.\",\n",
        "        \"Great direction and a touching story.\"\n",
        "    ],\n",
        "    'label': ['positive', 'negative', 'positive', 'negative', 'positive',\n",
        "              'negative', 'positive', 'positive', 'negative', 'positive']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Text Vectorization (TF-IDF)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Labels\n",
        "y = df['label']\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# --------- User Input Section ---------\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sentence to predict sentiment (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    input_vector = vectorizer.transform([user_input])  # Transform input using same vectorizer\n",
        "    prediction = model.predict(input_vector)\n",
        "    print(\"Predicted Sentiment:\", prediction[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8rjflSv0N8D",
        "outputId": "0f517559-f371-4bc0-82c2-25e100543174"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       2.0\n",
            "    positive       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "\n",
            "Enter a sentence to predict sentiment (or type 'exit' to quit): i love this movie\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Enter a sentence to predict sentiment (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Sample dataset with 3 classes\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love this product\",\n",
        "        \"This is the best thing ever!\",\n",
        "        \"I am very happy with the results\",\n",
        "        \"This is okay, not great but not bad\",\n",
        "        \"It is fine\",\n",
        "        \"Nothing special about it\",\n",
        "        \"I hate it\",\n",
        "        \"This is the worst purchase\",\n",
        "        \"Totally disappointed\"\n",
        "    ],\n",
        "    'label': [\n",
        "        'positive', 'positive', 'positive',\n",
        "        'neutral', 'neutral', 'neutral',\n",
        "        'negative', 'negative', 'negative'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Build pipeline: TF-IDF + Logistic Regression\n",
        "model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Predict user input\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sentence (or 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    prediction = model.predict([user_input])[0]\n",
        "    print(f\"Predicted Sentiment: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcB8Sxxk1FPx",
        "outputId": "d5abd1c9-8853-4364-f17f-55e4500b4c97"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00         1\n",
            "     neutral       0.00      0.00      0.00         1\n",
            "    positive       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.33      0.22         3\n",
            "weighted avg       0.17      0.33      0.22         3\n",
            "\n",
            "\n",
            "Enter a sentence (or 'exit' to quit): i dont like this\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Enter a sentence (or 'exit' to quit): i like this \n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Enter a sentence (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translator"
      ],
      "metadata": {
        "id": "uzd7Rjtc2RxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUeZCwfS2gJf",
        "outputId": "e001875d-bd16-4416-dc34-930d2ad58e35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from Translate) (8.1.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from Translate) (5.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from Translate) (2.32.3)\n",
            "Collecting libretranslatepy==2.1.1 (from Translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->Translate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->Translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->Translate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->Translate) (2025.1.31)\n",
            "Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: libretranslatepy, Translate\n",
            "Successfully installed Translate-3.6.1 libretranslatepy-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "\n",
        "text = input(\"Enter the text to translate: \")\n",
        "target_lang = input(\"Enter target language code (e.g., 'fr' for French): \")\n",
        "translator = Translator(from_lang='en', to_lang=target_lang)\n",
        "translation = translator.translate(text)\n",
        "print(f\"\\nTranslated text en â†’ {target_lang}): {translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeOGgPlk2T09",
        "outputId": "8ce75809-2de1-4f55-91b7-8d5f89165cbc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text to translate: i like ice cream\n",
            "Enter target language code (e.g., 'fr' for French): ml\n",
            "\n",
            "Translated text en â†’ ml): à´Žà´¨à´¿à´•àµà´•àµ à´à´¸àµà´•àµà´°àµ€à´‚ à´‡à´·àµà´Ÿà´®à´¾à´£àµ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "ynA4kV2F3SWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Read input from user\n",
        "text = input(\"Enter a line of text: \")\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Tokens after removing stopwords:\", filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsKATz-S3MG-",
        "outputId": "644bf893-fe52-439a-fcea-d261ddf07d6f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a line of text: heloo good morning my name in manu\n",
            "Tokens: ['heloo', 'good', 'morning', 'my', 'name', 'in', 'manu']\n",
            "Tokens after removing stopwords: ['heloo', 'good', 'morning', 'name', 'manu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Read input from user\n",
        "text = input(\"Enter a line of text: \")\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzKak-Nx3ejR",
        "outputId": "95f9ef9e-49a2-4e84-cde3-74f2865a3a62"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a line of text: those people were eating leaves\n",
            "Tokens: ['those', 'people', 'were', 'eating', 'leaves']\n",
            "Stemmed Tokens: ['those', 'peopl', 'were', 'eat', 'leav']\n",
            "Lemmatized Tokens: ['those', 'people', 'were', 'eating', 'leaf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOFFO9vb4ONj",
        "outputId": "9e0e2e2a-7f7c-4467-8d3c-441951d92478"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import contractions\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Helper: Get WordNet POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Helper: Get synonym\n",
        "def get_synonym(word, pos=None):\n",
        "    synsets = wordnet.synsets(word, pos=pos) if pos else wordnet.synsets(word)\n",
        "    if synsets:\n",
        "        lemmas = synsets[0].lemmas()\n",
        "        for lemma in lemmas:\n",
        "            if lemma.name().lower() != word.lower():\n",
        "                return lemma.name().replace('_', ' ')\n",
        "    return word\n",
        "\n",
        "# Helper: Get antonym\n",
        "def get_antonym(word):\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                return lemma.antonyms()[0].name().replace('_', ' ')\n",
        "    return word\n",
        "\n",
        "# --- Main Program ---\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "# Step 1: Expand contractions\n",
        "expanded_text = contractions.fix(text)\n",
        "print(\"\\nExpanded Text:\", expanded_text)\n",
        "\n",
        "# Step 2: Tokenize and POS tag\n",
        "tokens = word_tokenize(expanded_text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Step 3: Replace with synonyms and antonyms\n",
        "modified_tokens = []\n",
        "i = 0\n",
        "while i < len(tagged):\n",
        "    word, tag = tagged[i]\n",
        "    wn_pos = get_wordnet_pos(tag)\n",
        "    if word.lower() == \"not\" and i + 1 < len(tagged):\n",
        "        # Look for antonym of next word\n",
        "        next_word, next_tag = tagged[i + 1]\n",
        "        antonym = get_antonym(next_word)\n",
        "        modified_tokens.append(antonym)\n",
        "        i += 2  # Skip \"not\" and next word\n",
        "    else:\n",
        "        # Replace with synonym\n",
        "        synonym = get_synonym(word, wn_pos)\n",
        "        modified_tokens.append(synonym)\n",
        "        i += 1\n",
        "\n",
        "print(\"\\nModified Text:\")\n",
        "print(' '.join(modified_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkQ80cgv31_Y",
        "outputId": "bc107835-6c76-4e4a-bdea-d966261d28a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: i can't say if he is not happy\n",
            "\n",
            "Expanded Text: i cannot say if he is not happy\n",
            "\n",
            "Modified Text:\n",
            "iodine tin say if helium be unhappy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGEX"
      ],
      "metadata": {
        "id": "172QFqcl5jRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample input\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "# Define regex patterns and their replacements\n",
        "replacements = {\n",
        "    r\"\\bdog\\b\": \"cat\",            # Replace the word 'dog' with 'cat'\n",
        "    r\"\\brun(ning)?\\b\": \"walk\",    # Replace 'run' or 'running' with 'walk'\n",
        "    r\"\\b[A-Z]{2,}\\b\": \"ABBR\",     # Replace all uppercase words (abbreviations) with 'ABBR'\n",
        "    r\"\\d{4}\": \"YEAR\"              # Replace any 4-digit number with 'YEAR'\n",
        "}\n",
        "\n",
        "# Apply all replacements\n",
        "for pattern, repl in replacements.items():\n",
        "    text = re.sub(pattern, repl, text)\n",
        "\n",
        "# Output\n",
        "print(\"\\nModified text:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruTp3vnR5k7s",
        "outputId": "11fe1193-3b5e-4a21-9a3a-faacc7ce04ba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: the dog is running\n",
            "\n",
            "Modified text:\n",
            "the cat is walk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "text = input(\"Enter some text:\\n\")\n",
        "tokens = word_tokenize(text)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "word_list_corpus = sorted(set(filtered_words))\n",
        "\n",
        "print(\"\\nWord List Corpus:\")\n",
        "print(word_list_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IUqSdrv52hY",
        "outputId": "82e430f0-6355-4d7e-80d3-d0a4b31fe478"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter some text:\n",
            "the brown fox jumps over the crazy cat on his back\n",
            "\n",
            "Word List Corpus:\n",
            "['back', 'brown', 'cat', 'crazy', 'fox', 'jumps']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to create a part-of-speech tagged word corpus after tokenizing a line of text,\n",
        "#filtering out stopwords, performing lemmatization and then performing part-of-speech tagging.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Uncomment if running for the first time\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Function to convert POS tag to WordNet format for lemmatization\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Input text\n",
        "text = input(\"Enter a line of text:\\n\")\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "words = [w.lower() for w in tokens if w.isalpha() and w.lower() not in stopwords.words('english')]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(tag)) for w, tag in nltk.pos_tag(words)]\n",
        "\n",
        "# POS Tagging\n",
        "pos_tagged = nltk.pos_tag(lemmatized_words)\n",
        "\n",
        "# Output the result\n",
        "print(\"\\nPOS-Tagged Word Corpus:\")\n",
        "for word, tag in pos_tagged:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9rLMupU6T1H",
        "outputId": "cf1f4e93-a981-4772-805f-bde3d7c2fab5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a line of text:\n",
            "he was playing with ball\n",
            "\n",
            "POS-Tagged Word Corpus:\n",
            "play: NN\n",
            "ball: NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to tag proper names.\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "text = input(\"Enter a sentence: \")\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"\\nProper Names Tagged:\")\n",
        "for chunk in named_entities:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        print(f\"{chunk.label()}: {' '.join(c[0] for c in chunk)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J58cjKnt6zWZ",
        "outputId": "66335667-549f-4021-b6b3-fed0be3a0f88"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Barack was born in Hawaii\n",
            "\n",
            "Proper Names Tagged:\n",
            "PERSON: Barack\n",
            "GPE: Hawaii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Write a Python program to perform tagging using regular expressions.\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "# Sample text\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Define regular expression patterns for tagging\n",
        "patterns = [\n",
        "    (r'^[A-Z][a-z]+$', 'NNP'),       # Proper noun (initial capital)\n",
        "    (r'.*ing$', 'VBG'),              # Gerunds\n",
        "    (r'.*ed$', 'VBD'),               # Past tense verbs\n",
        "    (r'.*es$', 'VBZ'),               # 3rd person singular verbs\n",
        "    (r'.*ould$', 'MD'),              # Modals\n",
        "    (r'.*\\'s$', 'POS'),              # Possessive nouns\n",
        "    (r'.*s$', 'NNS'),                # Plural nouns\n",
        "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),# Cardinal numbers\n",
        "    (r'.*', 'NN')                    # Default noun\n",
        "]\n",
        "\n",
        "# Create the RegexpTagger\n",
        "regexp_tagger = RegexpTagger(patterns)\n",
        "\n",
        "# Tag the tokens\n",
        "tagged = regexp_tagger.tag(tokens)\n",
        "\n",
        "# Output the result\n",
        "print(\"\\nTagged Output:\")\n",
        "for word, tag in tagged:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFbGkx7F7Piv",
        "outputId": "41d0bd48-09d9-449d-bdea-2cd23a8e0cb2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: manu is playing\n",
            "\n",
            "Tagged Output:\n",
            "manu: NN\n",
            "is: NNS\n",
            "playing: VBG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag.sequential import ClassifierBasedTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "def feature_detector(tokens, index, history):\n",
        "    word = tokens[index]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(tokens) - 1,\n",
        "        'is_capitalized': word[0].upper() == word[0],\n",
        "        'is_all_caps': word.upper() == word,\n",
        "        'is_all_lower': word.lower() == word,\n",
        "        'prefix-1': word[0],\n",
        "        'prefix-2': word[:2],\n",
        "        'suffix-1': word[-1],\n",
        "        'suffix-2': word[-2:],\n",
        "        'prev_word': '' if index == 0 else tokens[index - 1],\n",
        "        'next_word': '' if index == len(tokens) - 1 else tokens[index + 1],\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Load training data\n",
        "train_sents = treebank.tagged_sents()[:3000]\n",
        "\n",
        "# Train the tagger\n",
        "print(\"Training classifier-based tagger...\")\n",
        "tagger = ClassifierBasedTagger(train=train_sents, feature_detector=feature_detector)\n",
        "print(\"Training complete!\\n\")\n",
        "\n",
        "# Input and processing\n",
        "text = input(\"Enter a sentence: \")\n",
        "tokens = word_tokenize(text)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Tagging\n",
        "tagged = tagger.tag(lemmatized)\n",
        "\n",
        "# Output\n",
        "print(\"\\nPOS Tags:\")\n",
        "for word, tag in tagged:\n",
        "    print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRLI-JbU76_y",
        "outputId": "9c10d3a6-87ef-46c9-a1b0-8e4d29f87848"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier-based tagger...\n",
            "Training complete!\n",
            "\n",
            "Enter a sentence: brown fox\n",
            "\n",
            "POS Tags:\n",
            "brown: JJ\n",
            "fox: NN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to create NER tagged word corpus.\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "# Input sentence\n",
        "text = input(\"Enter a sentence: \")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Perform Named Entity Recognition\n",
        "ner_tree = ne_chunk(pos_tags)\n",
        "print(ner_tree)\n",
        "\n",
        "# Display NER-tagged words\n",
        "print(\"\\nNER Tagged Word Corpus:\")\n",
        "for subtree in ner_tree:\n",
        "    if hasattr(subtree, 'label'):\n",
        "        # Named Entity (e.g., PERSON, ORGANIZATION)\n",
        "        entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
        "        print(f\"{entity_name}: {subtree.label()}\")\n",
        "    else:\n",
        "        # Not a named entity\n",
        "        word, tag = subtree\n",
        "        print(f\"{word}: {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bexnukij8tH-",
        "outputId": "4c2e9ed0-7ada-48e3-cbaa-9e6037ea928e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Obama in Hawaii\n",
            "(S (GPE Obama/NNP) in/IN (GPE Hawaii/NNP))\n",
            "\n",
            "NER Tagged Word Corpus:\n",
            "Obama: GPE\n",
            "in: IN\n",
            "Hawaii: GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python program to rank the words in a document using TF-IDF.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample input: list of documents (you can replace or modify this)\n",
        "documents = [\n",
        "    \"The movie was fantastic and full of emotions.\",\n",
        "    \"The film had great direction and brilliant acting.\",\n",
        "    \"What a terrible movie. I won't recommend it.\",\n",
        "    \"A fantastic plot with superb performance.\"\n",
        "]\n",
        "\n",
        "# Create TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame with TF-IDF scores\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Rank words in each document by TF-IDF score\n",
        "for idx, row in df.iterrows():\n",
        "    print(f\"\\nDocument {idx + 1} Word Rankings:\")\n",
        "    sorted_row = row.sort_values(ascending=False)\n",
        "    for word, score in sorted_row.items():\n",
        "        if score > 0:\n",
        "            print(f\"{word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoA6WBbd9Key",
        "outputId": "8395822e-ab9c-4076-c520-f0012e04fa15"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1 Word Rankings:\n",
            "emotions: 0.3926\n",
            "of: 0.3926\n",
            "full: 0.3926\n",
            "was: 0.3926\n",
            "movie: 0.3096\n",
            "fantastic: 0.3096\n",
            "and: 0.3096\n",
            "the: 0.3096\n",
            "\n",
            "Document 2 Word Rankings:\n",
            "acting: 0.3716\n",
            "brilliant: 0.3716\n",
            "direction: 0.3716\n",
            "film: 0.3716\n",
            "great: 0.3716\n",
            "had: 0.3716\n",
            "and: 0.2929\n",
            "the: 0.2929\n",
            "\n",
            "Document 3 Word Rankings:\n",
            "terrible: 0.4218\n",
            "won: 0.4218\n",
            "what: 0.4218\n",
            "recommend: 0.4218\n",
            "it: 0.4218\n",
            "movie: 0.3325\n",
            "\n",
            "Document 4 Word Rankings:\n",
            "with: 0.4652\n",
            "plot: 0.4652\n",
            "performance: 0.4652\n",
            "superb: 0.4652\n",
            "fantastic: 0.3667\n"
          ]
        }
      ]
    }
  ]
}